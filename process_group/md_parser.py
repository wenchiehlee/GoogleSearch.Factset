#!/usr/bin/env python3
"""
MD Parser - FactSet Pipeline v3.6.1 (Modified)
å¢å¼·ç‰ˆ MD æª”æ¡ˆè§£æå™¨ï¼Œå°ç¼ºå°‘å…§å®¹æ—¥æœŸçš„æª”æ¡ˆçµ¦äºˆä½å“è³ªè©•åˆ†
å®Œå…¨æ•´åˆ v3.6.1 åŠŸèƒ½è¦æ±‚
"""

import os
import sys
import re
import yaml
import pandas as pd
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
import statistics
import json
try:
    from quality_analyzer_simplified import QualityAnalyzerSimplified
except ImportError:
    from process_group.quality_analyzer_simplified import QualityAnalyzerSimplified

# Set UTF-8 encoding for Windows console (only if not already set)
if sys.platform == 'win32' and hasattr(sys.stdout, 'buffer'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

class MDParser:
    def __init__(self):
        """åˆå§‹åŒ– MD è§£æå™¨ - v3.6.1 å¢å¼·ç‰ˆ"""
        
        self.version = "3.6.2-md-date-migrate"
        
        # å¢å¼·çš„ metadata æ¨¡å¼ - æ”¯æ´æŸ¥è©¢æ¨¡å¼æå–
        self.metadata_patterns = {
            'search_query': r'search_query:\s*(.+?)(?:\n|$)',
            'keywords': r'keywords:\s*(.+?)(?:\n|$)',
            'search_terms': r'search_terms:\s*(.+?)(?:\n|$)',
            'quality_score': r'quality_score:\s*([0-9]+(?:\.[0-9]+)?)(?:\n|$)',  # å“è³ªè©•åˆ†
            'query_pattern': r'query_pattern:\s*(.+?)(?:\n|$)',
            'original_query': r'original_query:\s*(.+?)(?:\n|$)',
            'company_code': r'company_code:\s*(.+?)(?:\n|$)',
            'company_name': r'company_name:\s*(.+?)(?:\n|$)',
            'data_source': r'data_source:\s*(.+?)(?:\n|$)',
            'timestamp': r'timestamp:\s*(.+?)(?:\n|$)',
            'extracted_date': r'extracted_date:\s*(.+?)(?:\n|$)'
        }
        
        # åŸæœ‰çš„æ—¥æœŸå’Œæ•¸æ“šæå–æ¨¡å¼
        self.date_patterns = [
            r'\*\s*(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'\*\s*æ›´æ–°[ï¼š:]\s*(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'æ›´æ–°[ï¼š:]\s*(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'é‰…äº¨ç¶²æ–°èä¸­å¿ƒ\s*\n\s*\n\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?\s*[ä¸Šä¸‹]åˆ\s*\d{1,2}:\d{1,2}',
            r'é‰…äº¨ç¶²æ–°èä¸­å¿ƒ\s*\n\s*\n\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'é‰…äº¨ç¶²æ–°èä¸­å¿ƒ[\s\n]+(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?\s*[ä¸Šä¸‹]åˆ\s*\d{1,2}:\d{1,2}',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}:\d{1,2}',
            r'(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
        ]
        
        self.eps_patterns = [
            r'(\d{4})å¹´[^|]*\|\s*([0-9]+\.?[0-9]*)',
            r'(\d{4})\s*å¹´\s*[:ï¼š]?\s*([0-9]+\.?[0-9]*)',
            r'(\d{4})\s*eps\s*[é ä¼°é æ¸¬ä¼°ç®—]*\s*[:ï¼š]\s*([0-9]+\.?[0-9]*)',
            r'eps\s*(\d{4})\s*[:ï¼š]\s*([0-9]+\.?[0-9]*)',
            r'å¹³å‡å€¼[^|]*(\d{4})[^|]*\|\s*([0-9]+\.?[0-9]*)',
            r'ä¸­ä½æ•¸[^|]*(\d{4})[^|]*\|\s*([0-9]+\.?[0-9]*)',
        ]
        
        self.target_price_patterns = [
            r'ç›®æ¨™åƒ¹\s*[:ï¼šç‚º]\s*([0-9]+\.?[0-9]*)\s*å…ƒ',
            r'é ä¼°ç›®æ¨™åƒ¹\s*[:ï¼šç‚º]\s*([0-9]+\.?[0-9]*)\s*å…ƒ',
            r'target\s*price\s*[:ï¼š]\s*([0-9]+\.?[0-9]*)',
        ]
        
        self.revenue_patterns = [
            r'(\d{4})å¹´[^|]*\|\s*([0-9,]+(?:\.[0-9]+)?)',
            r'(\d{4})\s*å¹´\s*ç‡Ÿæ”¶\s*[:ï¼š]?\s*([0-9,]+(?:\.[0-9]+)?)',
            r'revenue\s*(\d{4})\s*[:ï¼š]\s*([0-9,]+(?:\.[0-9]+)?)',
        ]
        
        self.analyst_patterns = [
            r'å…±\s*(\d+)\s*ä½åˆ†æå¸«',
            r'(\d+)\s*ä½åˆ†æå¸«',
            r'(\d+)\s*analysts?',
        ]

        # è¼‰å…¥è§€å¯Ÿåå–®ä¸¦é€²è¡Œåš´æ ¼é©—è­‰
        self.watch_list_mapping = self._load_watch_list_mapping_enhanced()
        self.validation_enabled = len(self.watch_list_mapping) > 0

        # åˆå§‹åŒ–å“è³ªåˆ†æå™¨ (ç”¨æ–¼ç‰ˆæœ¬é·ç§»æ™‚é‡æ–°è¨ˆç®—åˆ†æ•¸)
        self.quality_analyzer = QualityAnalyzerSimplified()

        # å¼·åˆ¶é‡æ–°æƒææ¨™è¨˜ (ç”¨æ–¼ä¿®å¾©å·²é·ç§»ä½†åˆ†æ•¸ä¸æ­£ç¢ºçš„æª”æ¡ˆ)
        self.force_rescan = False

        print(f"MDParser v{self.version} åˆå§‹åŒ–å®Œæˆ")
        print(f"è§€å¯Ÿåå–®é©—è­‰: {'å•Ÿç”¨' if self.validation_enabled else 'åœç”¨'} ({len(self.watch_list_mapping)} å®¶å…¬å¸)")

    def _load_watch_list_mapping_enhanced(self) -> Dict[str, str]:
        """v3.6.1 å¢å¼·çš„è§€å¯Ÿåå–®è¼‰å…¥"""
        mapping = {}
        
        possible_paths = [
            'StockID_TWSE_TPEX.csv',
            '../StockID_TWSE_TPEX.csv',
            '../../StockID_TWSE_TPEX.csv',
            'data/StockID_TWSE_TPEX.csv',
            '../data/StockID_TWSE_TPEX.csv',
            'watchlist.csv',
            '../watchlist.csv'
        ]
        
        for csv_path in possible_paths:
            if os.path.exists(csv_path):
                try:
                    print(f"å˜—è©¦è¼‰å…¥è§€å¯Ÿåå–®: {csv_path}")
                    
                    # ä½¿ç”¨å¤šç¨®ç·¨ç¢¼å˜—è©¦è®€å–
                    encodings = ['utf-8', 'utf-8-sig', 'big5', 'gbk', 'cp950']
                    df = None
                    
                    for encoding in encodings:
                        try:
                            df = pd.read_csv(csv_path, header=0, encoding=encoding)
                            print(f"æˆåŠŸä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–")
                            break
                        except UnicodeDecodeError:
                            continue
                        except Exception as e:
                            print(f"ä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–å¤±æ•—: {e}")
                            continue

                    if df is None:
                        print(f"ç„¡æ³•ä½¿ç”¨ä»»ä½•ç·¨ç¢¼è®€å– {csv_path}")
                        continue

                    # åš´æ ¼é©—è­‰å’Œæ¸…ç†æ•¸æ“š
                    valid_count = 0
                    invalid_count = 0
                    duplicate_count = 0
                    skipped_count = 0  # è·³éçš„éæ•¸æ“šè¡Œï¼ˆå¦‚ 0000ï¼‰

                    for idx, row in df.iterrows():
                        try:
                            # æå–ä¸¦æ¸…ç†ä»£è™Ÿå’Œåç¨±
                            code = str(row['ä»£è™Ÿ']).strip()
                            name = str(row['åç¨±']).strip()

                            # è·³éä½”ä½ç¬¦å’Œæ¸¬è©¦æ•¸æ“šï¼ˆä¸è¨ˆå…¥çµ±è¨ˆï¼‰
                            if code in ['0', '0000', '9999', 'TEST', 'test']:
                                skipped_count += 1
                                continue
                            
                            # åš´æ ¼é©—è­‰å…¬å¸ä»£è™Ÿæ ¼å¼
                            if not self._is_valid_company_code(code):
                                print(f"ç„¡æ•ˆå…¬å¸ä»£è™Ÿæ ¼å¼: '{code}' (ç¬¬{idx+1}è¡Œ)")
                                invalid_count += 1
                                continue
                            
                            # é©—è­‰å…¬å¸åç¨±
                            if not self._is_valid_company_name(name):
                                print(f"ç„¡æ•ˆå…¬å¸åç¨±: '{name}' (ä»£è™Ÿ: {code}, ç¬¬{idx+1}è¡Œ)")
                                invalid_count += 1
                                continue
                            
                            # æª¢æŸ¥é‡è¤‡ä»£è™Ÿ
                            if code in mapping:
                                print(f"é‡è¤‡å…¬å¸ä»£è™Ÿ: {code} - åŸæœ‰: {mapping[code]}, æ–°çš„: {name}")
                                duplicate_count += 1
                                continue
                            
                            # æ·»åŠ åˆ°æ˜ å°„
                            mapping[code] = name
                            valid_count += 1
                            
                        except Exception as e:
                            print(f"è™•ç†ç¬¬{idx+1}è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                            invalid_count += 1
                            continue
                    
                    # é©—è­‰è¼‰å…¥çµæœ
                    total_rows = len(df)
                    data_rows = total_rows - skipped_count  # å¯¦éš›æ•¸æ“šè¡Œæ•¸ï¼ˆæ’é™¤ä½”ä½ç¬¦ï¼‰

                    if valid_count == 0:
                        print(f"è§€å¯Ÿåå–®ç„¡æœ‰æ•ˆæ•¸æ“š: {csv_path}")
                        continue

                    print(f"è§€å¯Ÿåå–®è¼‰å…¥çµ±è¨ˆ:")
                    print(f"   æª”æ¡ˆ: {csv_path}")
                    print(f"   ç¸½è¡Œæ•¸: {total_rows}")
                    print(f"   æœ‰æ•ˆæ•¸æ“š: {valid_count}")
                    print(f"   ç„¡æ•ˆæ•¸æ“š: {invalid_count}")
                    print(f"   é‡è¤‡æ•¸æ“š: {duplicate_count}")
                    if skipped_count > 0:
                        print(f"   è·³éæ•¸æ“š: {skipped_count} (ä½”ä½ç¬¦/æ¸¬è©¦)")
                    print(f"   æˆåŠŸç‡: {valid_count/data_rows*100:.1f}%")
                    
                    # é¡å¤–é©—è­‰ï¼šæª¢æŸ¥æ˜¯å¦æœ‰å·²çŸ¥çš„æ¸¬è©¦å…¬å¸
                    self._validate_watch_list_content_enhanced(mapping)
                    
                    return mapping
                    
                except Exception as e:
                    print(f"è®€å–è§€å¯Ÿåå–®å¤±æ•— {csv_path}: {e}")
                    continue
        
        # å¦‚æœè§€å¯Ÿåå–®è¼‰å…¥å¤±æ•—ï¼Œè¿”å›ç©ºå­—å…¸ä½†ä¸åœæ­¢ç³»çµ±
        print("æ‰€æœ‰è§€å¯Ÿåå–®è¼‰å…¥å˜—è©¦å‡å¤±æ•—")
        print("ç³»çµ±å°‡åœ¨ç„¡é©—è­‰æ¨¡å¼ä¸‹é‹è¡Œ")
        return {}

    def _check_and_migrate_version(self, file_path: str, yaml_data: Dict, force_rescan: bool = False) -> bool:
        """
        æª¢æŸ¥ MD æª”æ¡ˆç‰ˆæœ¬ï¼Œè‹¥éæ™‚å‰‡æ›´æ–° metadata

        Args:
            file_path: MD æª”æ¡ˆè·¯å¾‘
            yaml_data: æª”æ¡ˆçš„ YAML metadata
            force_rescan: æ˜¯å¦å¼·åˆ¶é‡æ–°æƒæ (å³ä½¿ç‰ˆæœ¬ç›¸åŒ)

        Returns:
            True è¡¨ç¤ºæœ‰æ›´æ–°, False è¡¨ç¤ºç„¡éœ€æ›´æ–°
        """
        file_version = yaml_data.get('version', 'unknown')

        print(f"[DEBUG] æª¢æŸ¥ç‰ˆæœ¬: æª”æ¡ˆ={file_version}, ç•¶å‰={self.version}, å¼·åˆ¶æƒæ={force_rescan}")

        # å¦‚æœç‰ˆæœ¬ç›¸åŒä¸”æœªå¼·åˆ¶æƒæï¼Œç„¡éœ€æ›´æ–°
        if file_version == self.version and not force_rescan:
            print(f"[DEBUG] ç‰ˆæœ¬ç›¸åŒï¼Œè·³éé·ç§»")
            return False

        if force_rescan:
            print(f"ğŸ”„ å¼·åˆ¶é‡æ–°æƒæ: {file_version}")
        else:
            print(f"ğŸ”„ åµæ¸¬åˆ°ç‰ˆæœ¬å·®ç•°: {file_version} â†’ {self.version}")

        try:
            # è®€å–æª”æ¡ˆå…§å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # é‡æ–°è¨ˆç®— quality_score (ä½¿ç”¨ç•¶å‰ç‰ˆæœ¬çš„é‚è¼¯)
            new_quality_score = self._recalculate_quality_score(content)

            # é‡æ–°æå– md_date (å„ªå…ˆ meta/JSON-LD/time)
            new_md_date = self._extract_md_date_from_meta(content)

            # æ›´æ–° frontmatter
            updated = self._update_md_frontmatter(
                file_path,
                content,
                yaml_data,
                new_quality_score,
                new_md_date
            )

            if updated:
                print(f"âœ… å·²æ›´æ–°æª”æ¡ˆç‰ˆæœ¬: {file_version} â†’ {self.version}, quality_score: {yaml_data.get('quality_score')} â†’ {new_quality_score}")
                return True

        except Exception as e:
            print(f"âš ï¸  ç‰ˆæœ¬é·ç§»å¤±æ•—: {e}")

        return False

    def _recalculate_quality_score(self, content: str) -> float:
        """ä½¿ç”¨ç•¶å‰ç‰ˆæœ¬é‚è¼¯é‡æ–°è¨ˆç®— quality_score (å®Œæ•´ç‰ˆ - ä½¿ç”¨ quality_analyzer_simplified.py)"""
        try:
            # æå–æ‰€æœ‰é—œéµæ•¸æ“š
            eps_data = self._extract_eps_data(content)
            eps_stats = self._calculate_eps_statistics(eps_data)

            revenue_stats = self._calculate_revenue_statistics(content)

            analyst_count = self._extract_analyst_count(content)
            target_price = self._extract_target_price(content)
            content_date = self._extract_content_date_bulletproof(content)

            # æ§‹å»º parsed_data å­—å…¸ (èˆ‡ quality_analyzer_simplified.py å…¼å®¹)
            parsed_data = {
                'company_code': 'unknown',  # ç‰ˆæœ¬é·ç§»æ™‚ç„¡æ³•å¾ content æå–
                'company_name': 'unknown',
                'content_date': content_date,
                'analyst_count': analyst_count,
                'target_price': target_price,
            }

            # æ·»åŠ  EPS çµ±è¨ˆæ•¸æ“š
            parsed_data.update(eps_stats)

            # æ·»åŠ ç‡Ÿæ”¶çµ±è¨ˆæ•¸æ“š
            parsed_data.update(revenue_stats)

            # ä½¿ç”¨å®Œæ•´çš„ quality_analyzer_simplified.py é€²è¡Œè©•åˆ†
            quality_result = self.quality_analyzer.analyze(parsed_data)
            quality_score = quality_result.get('quality_score', 0.0)

            print(f"âœ“ é‡æ–°è¨ˆç®— quality_score: {quality_score}")
            print(f"  - EPS years: {quality_result.get('summary_metrics', {}).get('eps_years_available', 0)}")
            print(f"  - Revenue years: {quality_result.get('summary_metrics', {}).get('revenue_years_available', 0)}")
            print(f"  - Analyst count: {analyst_count}")
            print(f"  - Component scores: EPS={quality_result.get('component_scores', {}).get('eps_quality', 0):.1f}, Revenue={quality_result.get('component_scores', {}).get('revenue_quality', 0):.1f}")

            return quality_score

        except Exception as e:
            print(f"âš ï¸  é‡æ–°è¨ˆç®— quality_score å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return 0.0

    def _update_md_frontmatter(self, file_path: str, content: str, yaml_data: Dict, new_quality_score: float, new_md_date: Optional[str] = None) -> bool:
        """æ›´æ–° MD æª”æ¡ˆçš„ YAML frontmatter"""
        try:
            # æ‰¾åˆ° YAML frontmatter çš„ç¯„åœ
            yaml_match = re.match(r'^---\n(.*?)\n---\n', content, re.DOTALL)
            if not yaml_match:
                print("âš ï¸  æ‰¾ä¸åˆ° YAML frontmatter")
                return False

            yaml_content = yaml_match.group(1)
            rest_content = content[yaml_match.end():]

            # æ›´æ–°ç‰ˆæœ¬å’Œ quality_score
            updated_yaml = re.sub(
                r'version:\s*[^\n]+',
                f'version: {self.version}',
                yaml_content
            )
            updated_yaml = re.sub(
                r'quality_score:\s*[^\n]+',
                f'quality_score: {new_quality_score}',
                updated_yaml
            )

            # Update md_date if a reliable meta date is available
            if new_md_date:
                if re.search(r'^md_date:\s*.*$', updated_yaml, re.M):
                    updated_yaml = re.sub(
                        r'^md_date:\s*[^\n]*',
                        f'md_date: {new_md_date}',
                        updated_yaml,
                        flags=re.M
                    )
                else:
                    updated_yaml += f'\nmd_date: {new_md_date}'

            # Fix malformed search_query if present
            search_query = yaml_data.get('search_query', '')
            if isinstance(search_query, str) and '"' in search_query:
                if not (search_query.startswith("'") and search_query.endswith("'")):
                    safe_query = f"'{search_query}'"
                    updated_yaml = re.sub(
                        r'search_query:\s*[^\n]+',
                        f'search_query: {safe_query}',
                        updated_yaml
                    )

            # æ·»åŠ æ›´æ–°æ™‚é–“æˆ³è¨˜
            update_timestamp = datetime.now().isoformat()
            if 'updated_date:' in updated_yaml:
                updated_yaml = re.sub(
                    r'updated_date:\s*[^\n]+',
                    f'updated_date: {update_timestamp}',
                    updated_yaml
                )
            else:
                updated_yaml += f'\nupdated_date: {update_timestamp}'

            # é‡çµ„å®Œæ•´å…§å®¹
            new_content = f'---\n{updated_yaml}\n---\n{rest_content}'

            # å¯«å›æª”æ¡ˆ
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(new_content)

            return True

        except Exception as e:
            print(f"âš ï¸  æ›´æ–° frontmatter å¤±æ•—: {e}")
            return False

    def parse_md_file(self, file_path: str) -> Dict[str, Any]:
        """v3.6.1 å¢å¼·ç‰ˆ MD æª”æ¡ˆè§£æ"""
        try:
            # è®€å–æª”æ¡ˆå…§å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åŸºæœ¬æª”æ¡ˆè³‡è¨Š
            file_info = self._extract_file_info(file_path)
            company_code = file_info.get('company_code', '')
            company_name = file_info.get('company_name', '')
            
            # å¢å¼·çš„ YAML front matter è§£æ
            yaml_data = self._extract_yaml_frontmatter_enhanced(content)

            # ç‰ˆæœ¬æª¢æŸ¥èˆ‡è‡ªå‹•é·ç§» (v3.6.1 æ–°åŠŸèƒ½)
            was_migrated = self._check_and_migrate_version(file_path, yaml_data, force_rescan=self.force_rescan)

            # å¦‚æœæª”æ¡ˆè¢«æ›´æ–°ï¼Œé‡æ–°è®€å–ä»¥ç²å¾—æ–°çš„ metadata
            if was_migrated:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                yaml_data = self._extract_yaml_frontmatter_enhanced(content)

            # CRITICAL: Read quality_score from MD file YAML (ä¸é‡æ–°è¨ˆç®—)
            # ç›´æ¥è®€å– Search Group å¯«å…¥çš„ quality_scoreï¼Œä½œç‚º å“è³ªè©•åˆ†
            quality_score_from_yaml = yaml_data.get('quality_score')
            if quality_score_from_yaml is not None:
                try:
                    data_richness = float(quality_score_from_yaml)
                    print(f"âœ“ ä½¿ç”¨ MD æª”æ¡ˆçš„ quality_score: {data_richness}")
                except (ValueError, TypeError):
                    print(f"âš ï¸  quality_score æ ¼å¼éŒ¯èª¤ï¼Œä½¿ç”¨é è¨­å€¼ 0")
                    data_richness = 0.0
            else:
                print(f"âš ï¸  MD æª”æ¡ˆç¼ºå°‘ quality_scoreï¼Œä½¿ç”¨é è¨­å€¼ 0")
                data_richness = 0.0

            # æŸ¥è©¢æ¨¡å¼æå– (v3.6.1 æ ¸å¿ƒåŠŸèƒ½)
            search_keywords = self._extract_search_keywords_enhanced(content, yaml_data)

            # æ ¸å¿ƒé©—è­‰ï¼šå°ç…§è§€å¯Ÿåå–® (å¢å¼·ç‰ˆ)
            validation_result = self._validate_against_watch_list_enhanced(company_code, company_name)

            # åŸæœ‰åŠŸèƒ½ï¼šæ—¥æœŸæå–
            content_date = self._extract_content_date_bulletproof(content)
            extraction_status = "content_extraction" if content_date else "no_date_found"

            # åŸæœ‰åŠŸèƒ½ï¼šEPS ç­‰è³‡æ–™æå–
            eps_data = self._extract_eps_data(content)
            eps_stats = self._calculate_eps_statistics(eps_data)
            revenue_stats = self._calculate_revenue_statistics(content)
            target_price = self._extract_target_price(content)
            analyst_count = self._extract_analyst_count(content)

            # FIXED: ä¸å†é‡æ–°è¨ˆç®—å“è³ªè©•åˆ†ï¼Œç›´æ¥ä½¿ç”¨å¾ YAML è®€å–çš„å€¼
            # Process Group æ‡‰è©²åªè®€å– Search Group å¯«å…¥çš„ quality_scoreï¼Œä¿æŒå…©éšæ®µæ¶æ§‹åˆ†é›¢
            
            # å…§å®¹å“è³ªè©•ä¼° (v3.6.1)
            content_quality_metrics = self._assess_content_quality(content)
            
            # çµ„åˆçµæœ
            result = {
                # åŸºæœ¬è³‡è¨Š
                'filename': os.path.basename(file_path),
                'company_code': company_code,
                'company_name': company_name,
                'data_source': file_info.get('data_source', ''),
                'file_mtime': datetime.fromtimestamp(os.path.getmtime(file_path)),
                
                # æ—¥æœŸè³‡è¨Š
                'content_date': content_date,
                'extracted_date': yaml_data.get('extracted_date'),
                'filename_date': file_info.get('parsed_timestamp'),
                
                # è²¡å‹™è³‡æ–™
                **eps_stats,
                **revenue_stats,
                'target_price': target_price,
                'analyst_count': analyst_count,
                
                # è³‡æ–™ç‹€æ…‹
                'has_eps_data': len(eps_data) > 0,
                'has_target_price': target_price is not None,
                'has_analyst_info': analyst_count > 0,
                'data_richness_score': data_richness,
                'quality_score': data_richness,  # For backwards compatibility
                
                # v3.6.1 å¢å¼·åŠŸèƒ½
                'search_keywords': search_keywords,  # é—œéµï¼šæŸ¥è©¢æ¨¡å¼åˆ†æéœ€è¦
                'content_quality_metrics': content_quality_metrics,
                
                # YAML è³‡æ–™
                'yaml_data': yaml_data,
                
                # å¢å¼·ç‰ˆé©—è­‰çµæœ
                'validation_result': validation_result,
                'content_validation_passed': validation_result['overall_status'] == 'valid',
                'validation_warnings': validation_result.get('warnings', []),
                'validation_errors': validation_result.get('errors', []),
                'validation_enabled': self.validation_enabled,
                
                # åŸå§‹å…§å®¹
                'content': content,
                'content_length': len(content),
                'parsed_at': datetime.now(),
                
                # èª¿è©¦è³‡è¨Š
                'parser_version': self.version,
                'date_extraction_method': extraction_status,
                'debug_info': self._get_debug_info_enhanced(content, content_date, search_keywords)
            }
            
            return result
            
        except Exception as e:
            print(f"è§£ææª”æ¡ˆå¤±æ•— {file_path}: {e}")
            return self._create_empty_result_enhanced(file_path, str(e))

    def _calculate_data_richness_enhanced(self, eps_stats: Dict, revenue_stats: Dict, target_price: Optional[float], 
                                        analyst_count: int, content_date: str) -> float:
        """MODIFIED: è¨ˆç®—è³‡æ–™è±å¯Œåº¦åˆ†æ•¸ (0-10) - åŒ…å« EPS å’Œ ç‡Ÿæ”¶ çš„å®Œæ•´æ€§è©•ä¼°"""
        
        # CRITICAL: Content date availability check
        if not content_date or content_date.strip() == "":
            print(f"âš ï¸  ç¼ºå°‘å…§å®¹æ—¥æœŸï¼Œå“è³ªè©•åˆ†é™åˆ¶ç‚º1åˆ†")
            return 1.0
        
        score = 2.0  # Base score for having content date
        
        # Target price (1.0)
        if target_price is not None:
            score += 1.0
        
        # Analyst count (1.0)
        if analyst_count > 0:
            score += 1.0
            
        # EPS scoring (3.0 max, 1.0 per year present, capped at 3.0)
        eps_available = 0
        for year in ['2025', '2026', '2027', '2028']:
            if eps_stats.get(f'eps_{year}_avg') is not None or eps_stats.get(f'eps_{year}_median') is not None:
                eps_available += 0.5
            if eps_stats.get(f'eps_{year}_high') is not None or eps_stats.get(f'eps_{year}_low') is not None:
                eps_available += 0.5
        score += min(eps_available, 3.0)

        # Revenue scoring (3.0 max, 1.0 per year present, capped at 3.0)
        revenue_available = 0
        for year in ['2025', '2026', '2027', '2028']:
            if revenue_stats.get(f'revenue_{year}_avg') is not None or revenue_stats.get(f'revenue_{year}_median') is not None:
                revenue_available += 0.5
            if revenue_stats.get(f'revenue_{year}_high') is not None or revenue_stats.get(f'revenue_{year}_low') is not None:
                revenue_available += 0.5
        score += min(revenue_available, 3.0)
        
        return round(min(score, 10), 2)

    def parse_md_file(self, file_path: str) -> Dict[str, Any]:
        """v3.6.1 å¢å¼·ç‰ˆ MD æª”æ¡ˆè§£æ"""
        try:
            # è®€å–æª”æ¡ˆå…§å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åŸºæœ¬æª”æ¡ˆè³‡è¨Š
            file_info = self._extract_file_info(file_path)
            company_code = file_info.get('company_code', '')
            company_name = file_info.get('company_name', '')
            
            # å¢å¼·çš„ YAML front matter è§£æ
            yaml_data = self._extract_yaml_frontmatter_enhanced(content)

            # ç‰ˆæœ¬æª¢æŸ¥èˆ‡è‡ªå‹•é·ç§» (v3.6.1 æ–°åŠŸèƒ½)
            was_migrated = self._check_and_migrate_version(file_path, yaml_data, force_rescan=self.force_rescan)

            # å¦‚æœæª”æ¡ˆè¢«æ›´æ–°ï¼Œé‡æ–°è®€å–ä»¥ç²å¾—æ–°çš„ metadata
            if was_migrated:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                yaml_data = self._extract_yaml_frontmatter_enhanced(content)

            # æŸ¥è©¢æ¨¡å¼æå–
            search_keywords = self._extract_search_keywords_enhanced(content, yaml_data)

            # æ ¸å¿ƒé©—è­‰
            validation_result = self._validate_against_watch_list_enhanced(company_code, company_name)

            # æ—¥æœŸæå–
            content_date = self._extract_content_date_bulletproof(content)
            extraction_status = "content_extraction" if content_date else "no_date_found"

            # è³‡æ–™æå–
            eps_data = self._extract_eps_data(content)
            eps_stats = self._calculate_eps_statistics(eps_data)
            revenue_stats = self._calculate_revenue_statistics(content)
            target_price = self._extract_target_price(content)
            analyst_count = self._extract_analyst_count(content)

            # CRITICAL: Read quality_score from MD file YAML (ä¸é‡æ–°è¨ˆç®—)
            # ç›´æ¥è®€å– Search Group å¯«å…¥çš„ quality_scoreï¼Œä½œç‚º å“è³ªè©•åˆ†
            quality_score_from_yaml = yaml_data.get('quality_score')
            if quality_score_from_yaml is not None:
                try:
                    data_richness = float(quality_score_from_yaml)
                except (ValueError, TypeError):
                    data_richness = 0.0
            else:
                data_richness = 0.0

            # å…§å®¹å“è³ªè©•ä¼°
            content_quality_metrics = self._assess_content_quality(content)
            
            # çµ„åˆçµæœ
            result = {
                'filename': os.path.basename(file_path),
                'company_code': company_code,
                'company_name': company_name,
                'data_source': file_info.get('data_source', ''),
                'file_mtime': datetime.fromtimestamp(os.path.getmtime(file_path)),
                
                'content_date': content_date,
                'extracted_date': yaml_data.get('extracted_date'),
                'filename_date': file_info.get('parsed_timestamp'),
                
                **eps_stats,
                **revenue_stats,
                'target_price': target_price,
                'analyst_count': analyst_count,
                
                'has_eps_data': len(eps_data) > 0,
                'has_target_price': target_price is not None,
                'has_analyst_info': analyst_count > 0,
                'data_richness_score': data_richness,
                'quality_score': data_richness,
                
                'search_keywords': search_keywords,
                'content_quality_metrics': content_quality_metrics,
                'yaml_data': yaml_data,
                'validation_result': validation_result,
                'content_validation_passed': validation_result['overall_status'] == 'valid',
                'validation_warnings': validation_result.get('warnings', []),
                'validation_errors': validation_result.get('errors', []),
                'validation_enabled': self.validation_enabled,
                'content': content,
                'content_length': len(content),
                'parsed_at': datetime.now(),
                'parser_version': self.version,
                'date_extraction_method': extraction_status,
                'debug_info': self._get_debug_info_enhanced(content, content_date, search_keywords)
            }
            
            return result
            
        except Exception as e:
            print(f"è§£ææª”æ¡ˆå¤±æ•— {file_path}: {e}")
            return self._create_empty_result_enhanced(file_path, str(e))

    def _extract_search_keywords_enhanced(self, content: str, yaml_data: Dict) -> List[str]:
        """v3.6.1 å¢å¼·çš„æœå°‹é—œéµå­—æå–"""
        keywords = []
        
        try:
            # 1. å¾ YAML metadata ä¸­æå–
            for field_name in ['search_query', 'keywords', 'search_terms', 'query_pattern', 'original_query']:
                field_value = yaml_data.get(field_name, '')
                if field_value and isinstance(field_value, str):
                    # æ¸…ç†å’Œåˆ†å‰²é—œéµå­—
                    cleaned_keywords = self._clean_and_split_keywords(field_value)
                    keywords.extend(cleaned_keywords)
            
            # 2. å¾å…§å®¹çš„ metadata ä¸­æå–
            if content.startswith('---'):
                try:
                    end_pos = content.find('---', 3)
                    if end_pos != -1:
                        yaml_content = content[3:end_pos].strip()
                        
                        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–æŸ¥è©¢ç›¸é—œæ¬„ä½
                        for field_name, pattern in self.metadata_patterns.items():
                            matches = re.findall(pattern, yaml_content, re.MULTILINE | re.IGNORECASE)
                            for match in matches:
                                if match.strip():
                                    cleaned_keywords = self._clean_and_split_keywords(match.strip())
                                    keywords.extend(cleaned_keywords)
                except Exception as e:
                    print(f"YAML metadata è§£æå¤±æ•—: {e}")
            
            # 3. å»é‡ä¸¦éæ¿¾
            unique_keywords = []
            seen_keywords = set()
            
            for keyword in keywords:
                keyword_lower = keyword.lower().strip()
                if (keyword_lower not in seen_keywords and 
                    self._is_valid_keyword(keyword_lower) and
                    len(keyword_lower) >= 2):
                    unique_keywords.append(keyword.strip())
                    seen_keywords.add(keyword_lower)
            
            # 4. æŒ‰é‡è¦æ€§æ’åº
            sorted_keywords = self._sort_keywords_by_importance(unique_keywords)
            
            return sorted_keywords[:20]  # é™åˆ¶æœ€å¤š20å€‹é—œéµå­—
            
        except Exception as e:
            print(f"æœå°‹é—œéµå­—æå–å¤±æ•—: {e}")
            return []

    def _clean_and_split_keywords(self, text: str) -> List[str]:
        """æ¸…ç†å’Œåˆ†å‰²é—œéµå­—"""
        if not text or not isinstance(text, str):
            return []
        
        # ç§»é™¤å¸¸è¦‹çš„æœå°‹é‹ç®—ç¬¦
        cleaned = re.sub(r'[+\-"():]', ' ', text)
        
        # åˆ†å‰²é—œéµå­— (æ”¯æ´å¤šç¨®åˆ†éš”ç¬¦)
        keywords = re.split(r'[,ï¼Œ;ï¼›\s]+', cleaned)
        
        # æ¸…ç†æ¯å€‹é—œéµå­—
        cleaned_keywords = []
        for keyword in keywords:
            keyword = keyword.strip()
            if keyword and len(keyword) >= 2:
                cleaned_keywords.append(keyword)
        
        return cleaned_keywords

    def _is_valid_keyword(self, keyword: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆé—œéµå­—"""
        if not keyword or len(keyword) < 2:
            return False
        
        # åœç”¨è©åˆ—è¡¨
        stop_words = {
            'çš„', 'å’Œ', 'èˆ‡', 'æˆ–', 'åŠ', 'åœ¨', 'ç‚º', 'æ˜¯', 'æœ‰', 'æ­¤', 'å°‡', 'æœƒ', 'äº†', 'å°±', 'éƒ½',
            'and', 'or', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with'
        }
        
        if keyword.lower() in stop_words:
            return False
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç´”æ•¸å­—æˆ–ç¬¦è™Ÿ
        if keyword.isdigit() or not re.search(r'[a-zA-Z\u4e00-\u9fff]', keyword):
            return False
        
        return True

    def _sort_keywords_by_importance(self, keywords: List[str]) -> List[str]:
        """æŒ‰é‡è¦æ€§æ’åºé—œéµå­—"""
        def get_importance_score(keyword: str) -> int:
            score = 0
            keyword_lower = keyword.lower()
            
            # é«˜é‡è¦æ€§é—œéµå­—
            high_importance = ['factset', 'eps', 'ç›®æ¨™åƒ¹', 'åˆ†æå¸«', 'é ä¼°', 'bloomberg', 'reuters']
            if any(imp in keyword_lower for imp in high_importance):
                score += 10
            
            # ä¸­é‡è¦æ€§é—œéµå­—
            medium_importance = ['è²¡å ±', 'ç‡Ÿæ”¶', 'ç²åˆ©', 'è‚¡åƒ¹', 'è©•ç­‰', 'analyst', 'forecast', 'estimate']
            if any(imp in keyword_lower for imp in medium_importance):
                score += 5
            
            # å…¬å¸åç¨±å’Œä»£è™Ÿ
            if re.search(r'\d{4}', keyword) or len(keyword) <= 4:
                score += 8
            
            # ä¸­æ–‡é—œéµå­—ç¨å¾®æé«˜å„ªå…ˆç´š
            if re.search(r'[\u4e00-\u9fff]', keyword):
                score += 2
            
            return score
        
        return sorted(keywords, key=get_importance_score, reverse=True)

    def _assess_content_quality(self, content: str) -> Dict[str, Any]:
        """v3.6.1 è©•ä¼°å…§å®¹å“è³ª"""
        metrics = {
            'content_length': len(content),
            'paragraph_count': len(content.split('\n\n')),
            'line_count': len(content.split('\n')),
            'chinese_char_ratio': 0,
            'financial_keyword_count': 0,
            'table_count': 0,
            'number_count': 0,
            'has_metadata': content.startswith('---'),
            'structure_score': 0
        }
        
        try:
            # ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
            if metrics['content_length'] > 0:
                metrics['chinese_char_ratio'] = round(chinese_chars / metrics['content_length'], 3)
            
            # è²¡å‹™é—œéµå­—è¨ˆæ•¸
            financial_keywords = [
                'eps', 'æ¯è‚¡ç›ˆé¤˜', 'ç‡Ÿæ”¶', 'ç²åˆ©', 'æ·¨åˆ©', 'æ¯›åˆ©', 'ç›®æ¨™åƒ¹', 'åˆ†æå¸«', 
                'é ä¼°', 'è©•ç­‰', 'factset', 'bloomberg', 'è‚¡åƒ¹', 'å¸‚å€¼'
            ]
            for keyword in financial_keywords:
                metrics['financial_keyword_count'] += len(re.findall(keyword, content, re.IGNORECASE))
            
            # è¡¨æ ¼è¨ˆæ•¸
            metrics['table_count'] = content.count('|')
            
            # æ•¸å­—è¨ˆæ•¸
            metrics['number_count'] = len(re.findall(r'\d+\.?\d*', content))
            
            # çµæ§‹è©•åˆ† (0-10)
            structure_score = 0
            if metrics['has_metadata']:
                structure_score += 3
            if metrics['paragraph_count'] >= 3:
                structure_score += 2
            if metrics['table_count'] > 0:
                structure_score += 2
            if metrics['financial_keyword_count'] >= 5:
                structure_score += 2
            if metrics['content_length'] >= 1000:
                structure_score += 1
            
            metrics['structure_score'] = min(structure_score, 10)
            
        except Exception as e:
            print(f"å…§å®¹å“è³ªè©•ä¼°å¤±æ•—: {e}")
        
        return metrics

    # Include all other existing methods (unchanged)
    def _extract_yaml_frontmatter_enhanced(self, content: str) -> Dict[str, Any]:
        """v3.6.1 å¢å¼·çš„ YAML front matter æå–"""
        try:
            if content.startswith('---'):
                end_pos = content.find('---', 3)
                if end_pos != -1:
                    yaml_content = content[3:end_pos].strip()
                    
                    # å˜—è©¦ä½¿ç”¨ yaml è§£æå™¨
                    try:
                        yaml_data = yaml.safe_load(yaml_content) or {}
                        
                        # é¡å¤–æ¸…ç†å’Œé©—è­‰
                        cleaned_data = {}
                        for key, value in yaml_data.items():
                            if isinstance(value, str):
                                cleaned_data[key] = value.strip()
                            else:
                                cleaned_data[key] = value
                        
                        return cleaned_data
                        
                    except yaml.YAMLError as e:
                        print(f"YAML è§£æå¤±æ•—ï¼Œå˜—è©¦æ‰‹å‹•è§£æ: {e}")
                        
                        # æ‰‹å‹•è§£æé—œéµæ¬„ä½
                        manual_data = {}
                        for field_name, pattern in self.metadata_patterns.items():
                            matches = re.findall(pattern, yaml_content, re.MULTILINE | re.IGNORECASE)
                            if matches:
                                manual_data[field_name] = matches[0].strip()
                        
                        return manual_data
                        
        except Exception as e:
            print(f"YAML frontmatter æå–å¤±æ•—: {e}")
        
        return {}

    def _validate_against_watch_list_enhanced(self, company_code: str, company_name: str) -> Dict[str, Any]:
        """v3.6.1 å¢å¼·çš„è§€å¯Ÿåå–®é©—è­‰"""
        
        validation_result = {
            'overall_status': 'valid',
            'warnings': [],
            'errors': [],
            'confidence_score': 10.0,
            'validation_method': 'enhanced_v3.6.1',
            'detailed_checks': []
        }
        
        # å¦‚æœè§€å¯Ÿåå–®æœªè¼‰å…¥ï¼Œè¨˜éŒ„ä½†ä¸é˜»æ­¢è™•ç†
        if not self.validation_enabled:
            validation_result['warnings'].append("è§€å¯Ÿåå–®æœªè¼‰å…¥ï¼Œè·³éé©—è­‰")
            validation_result['confidence_score'] = 5.0
            validation_result['validation_method'] = 'disabled'
            validation_result['detailed_checks'].append("é©—è­‰åŠŸèƒ½å·²åœç”¨")
            print(f"è§€å¯Ÿåå–®é©—è­‰å·²åœç”¨: {company_code} - {company_name}")
            return validation_result
        
        # åš´æ ¼æª¢æŸ¥è¼¸å…¥åƒæ•¸
        if not company_code or not company_name:
            validation_result['overall_status'] = 'error'
            validation_result['confidence_score'] = 0.0
            error_msg = f"å…¬å¸ä»£è™Ÿæˆ–åç¨±ç‚ºç©º: ä»£è™Ÿ='{company_code}', åç¨±='{company_name}'"
            validation_result['errors'].append(error_msg)
            validation_result['detailed_checks'].append("è¼¸å…¥åƒæ•¸æª¢æŸ¥å¤±æ•—")
            print(f"åƒæ•¸éŒ¯èª¤: {error_msg}")
            return validation_result
        
        # æ¸…ç†è¼¸å…¥æ•¸æ“š
        clean_code = str(company_code).strip().strip('\'"')
        clean_name = str(company_name).strip()
        
        # æª¢æŸ¥ 1: å…¬å¸ä»£è™Ÿæ ¼å¼é©—è­‰
        validation_result['detailed_checks'].append("æª¢æŸ¥å…¬å¸ä»£è™Ÿæ ¼å¼")
        if not self._is_valid_company_code(clean_code):
            validation_result['overall_status'] = 'error'
            validation_result['confidence_score'] = 0.0
            error_msg = f"å…¬å¸ä»£è™Ÿæ ¼å¼ç„¡æ•ˆ: '{clean_code}'"
            validation_result['errors'].append(error_msg)
            validation_result['detailed_checks'].append("å…¬å¸ä»£è™Ÿæ ¼å¼æª¢æŸ¥å¤±æ•—")
            print(f"ä»£è™Ÿæ ¼å¼ç„¡æ•ˆ: {clean_code}")
            return validation_result
        
        # æª¢æŸ¥ 2: å…¬å¸ä»£è™Ÿæ˜¯å¦åœ¨è§€å¯Ÿåå–®ä¸­
        validation_result['detailed_checks'].append("æª¢æŸ¥è§€å¯Ÿåå–®åŒ…å«ç‹€æ…‹")
        if clean_code not in self.watch_list_mapping:
            validation_result['overall_status'] = 'error'
            validation_result['confidence_score'] = 0.0
            error_msg = f"ä»£è™Ÿ{clean_code}ä¸åœ¨è§€å¯Ÿåå–®ä¸­ï¼Œä¸å…è¨±è™•ç†"
            validation_result['errors'].append(error_msg)
            validation_result['detailed_checks'].append("è§€å¯Ÿåå–®åŒ…å«æª¢æŸ¥å¤±æ•—")
            print(f"ä¸åœ¨è§€å¯Ÿåå–®: {clean_code}")
            
            # é¡å¤–ä¿¡æ¯ï¼šæä¾›ç›¸ä¼¼çš„ä»£è™Ÿå»ºè­°
            similar_codes = self._find_similar_codes(clean_code)
            if similar_codes:
                suggestion_msg = f"ç›¸ä¼¼ä»£è™Ÿå»ºè­°: {', '.join(similar_codes[:3])}"
                validation_result['warnings'].append(suggestion_msg)
                validation_result['detailed_checks'].append(f"æ‰¾åˆ°ç›¸ä¼¼ä»£è™Ÿ: {len(similar_codes)}å€‹")
            
            return validation_result
        
        # æª¢æŸ¥ 3: å…¬å¸åç¨±æ˜¯å¦èˆ‡è§€å¯Ÿåå–®ä¸€è‡´ (å¢å¼·æ¯”è¼ƒ)
        validation_result['detailed_checks'].append("æª¢æŸ¥å…¬å¸åç¨±ä¸€è‡´æ€§")
        correct_name = self.watch_list_mapping[clean_code]
        
        # å¤šå±¤æ¬¡åç¨±æ¯”è¼ƒ
        name_match = self._compare_company_names_enhanced(clean_name, correct_name)
        
        if not name_match['is_match']:
            validation_result['overall_status'] = 'error'
            validation_result['confidence_score'] = 0.0
            error_msg = f"å…¬å¸åç¨±ä¸ç¬¦è§€å¯Ÿåå–®ï¼šæª”æ¡ˆç‚º{clean_name}({clean_code})ï¼Œè§€å¯Ÿåå–®é¡¯ç¤ºæ‡‰ç‚º{correct_name}({clean_code})"
            validation_result['errors'].append(error_msg)
            validation_result['detailed_checks'].append("å…¬å¸åç¨±ä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—")
            
            # é¡å¤–ä¿¡æ¯ï¼šè©³ç´°çš„ä¸åŒ¹é…åˆ†æ
            if name_match['details']:
                validation_result['errors'].append(f"è©³ç´°æ¯”è¼ƒ: {name_match['details']}")
                validation_result['detailed_checks'].append(f"åç¨±æ¯”è¼ƒè©³æƒ…: {name_match['match_type']}")
            
            print(f"åç¨±ä¸ç¬¦: {clean_code}")
            print(f"   æª”æ¡ˆåç¨±: '{clean_name}'")
            print(f"   è§€å¯Ÿåå–®: '{correct_name}'")
            print(f"   æ¯”è¼ƒè©³æƒ…: {name_match['details']}")
            
            return validation_result
        
        # æª¢æŸ¥é€šé
        validation_result['confidence_score'] = name_match['confidence_score']
        validation_result['detailed_checks'].append(f"æ‰€æœ‰æª¢æŸ¥é€šéï¼Œåç¨±åŒ¹é…é¡å‹: {name_match['match_type']}")
        
        if name_match['confidence_score'] < 10.0:
            validation_result['warnings'].append(f"åç¨±åŒ¹é…åº¦: {name_match['confidence_score']}/10")
        
        print(f"é©—è­‰é€šé: {clean_code} - {clean_name} (ä¿¡å¿ƒåº¦: {name_match['confidence_score']})")
        return validation_result

    def _compare_company_names_enhanced(self, name1: str, name2: str) -> Dict[str, Any]:
        """v3.6.1 å¢å¼·çš„å…¬å¸åç¨±æ¯”è¼ƒ"""
        comparison_result = {
            'is_match': False,
            'confidence_score': 0.0,
            'details': '',
            'match_type': 'no_match'
        }
        
        # å±¤æ¬¡ 1: å®Œå…¨åŒ¹é…
        if name1 == name2:
            comparison_result.update({
                'is_match': True,
                'confidence_score': 10.0,
                'details': 'å®Œå…¨åŒ¹é…',
                'match_type': 'exact_match'
            })
            return comparison_result
        
        # å±¤æ¬¡ 2: ç§»é™¤ç©ºç™½å¾ŒåŒ¹é…
        clean_name1 = re.sub(r'\s+', '', name1)
        clean_name2 = re.sub(r'\s+', '', name2)
        
        if clean_name1 == clean_name2:
            comparison_result.update({
                'is_match': True,
                'confidence_score': 9.5,
                'details': 'ç§»é™¤ç©ºç™½å¾ŒåŒ¹é…',
                'match_type': 'whitespace_normalized'
            })
            return comparison_result
        
        # å±¤æ¬¡ 3: ç§»é™¤å¸¸è¦‹å¾Œç¶´è©å¾ŒåŒ¹é…
        suffixes = ['è‚¡ä»½æœ‰é™å…¬å¸', 'æœ‰é™å…¬å¸', 'å…¬å¸', 'é›†åœ˜', 'æ§è‚¡', 'Corporation', 'Corp', 'Inc', 'Ltd', 'Group']
        
        def remove_suffixes(name):
            for suffix in suffixes:
                if name.endswith(suffix):
                    name = name[:-len(suffix)].strip()
            return name
        
        core_name1 = remove_suffixes(clean_name1)
        core_name2 = remove_suffixes(clean_name2)
        
        if core_name1 == core_name2:
            comparison_result.update({
                'is_match': True,
                'confidence_score': 9.0,
                'details': 'ç§»é™¤å¾Œç¶´è©å¾ŒåŒ¹é…',
                'match_type': 'suffix_removed'
            })
            return comparison_result
        
        # å±¤æ¬¡ 4: éƒ¨åˆ†åŒ…å«åŒ¹é…
        if core_name1 in core_name2 or core_name2 in core_name1:
            comparison_result.update({
                'is_match': True,
                'confidence_score': 7.0,
                'details': f'éƒ¨åˆ†åŒ…å«åŒ¹é…: "{core_name1}" vs "{core_name2}"',
                'match_type': 'partial_contain'
            })
            return comparison_result
        
        # å±¤æ¬¡ 5: ç·¨è¼¯è·é›¢åŒ¹é… (æ–°å¢)
        similarity = self._calculate_string_similarity(core_name1, core_name2)
        if similarity >= 0.8:  # 80% ç›¸ä¼¼åº¦
            comparison_result.update({
                'is_match': True,
                'confidence_score': round(similarity * 6, 1),  # æœ€é«˜6åˆ†
                'details': f'é«˜ç›¸ä¼¼åº¦åŒ¹é…: {similarity:.2f}',
                'match_type': 'high_similarity'
            })
            return comparison_result
        
        # ä¸åŒ¹é…
        comparison_result.update({
            'details': f'å®Œå…¨ä¸åŒ¹é…: "{name1}" vs "{name2}" (ç›¸ä¼¼åº¦: {similarity:.2f})',
            'match_type': 'no_match'
        })
        return comparison_result

    def _calculate_string_similarity(self, str1: str, str2: str) -> float:
        """è¨ˆç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ (ç°¡åŒ–ç‰ˆç·¨è¼¯è·é›¢)"""
        try:
            if not str1 or not str2:
                return 0.0
            
            if str1 == str2:
                return 1.0
            
            # ç°¡åŒ–çš„ç›¸ä¼¼åº¦è¨ˆç®—
            longer = str1 if len(str1) > len(str2) else str2
            shorter = str2 if len(str1) > len(str2) else str1
            
            # è¨ˆç®—å…±åŒå­—ç¬¦
            common_chars = sum(1 for char in shorter if char in longer)
            similarity = common_chars / len(longer)
            
            return similarity
            
        except Exception:
            return 0.0

    # Include all other existing helper methods unchanged
    def _is_valid_company_code(self, code: str) -> bool:
        """é©—è­‰å…¬å¸ä»£è™Ÿæ ¼å¼"""
        if not code or code in ['nan', 'NaN', 'null', 'None', '', 'NULL']:
            return False
        
        clean_code = code.strip().strip('\'"')
        
        if not clean_code.isdigit():
            return False
            
        if len(clean_code) != 4:
            return False
        
        try:
            code_num = int(clean_code)
            if not (1000 <= code_num <= 9999):
                return False
        except ValueError:
            return False
        
        return True

    def _is_valid_company_name(self, name: str) -> bool:
        """é©—è­‰å…¬å¸åç¨±"""
        if not name or name in ['nan', 'NaN', 'null', 'None', '', 'NULL']:
            return False
        
        clean_name = name.strip()
        
        if len(clean_name) < 1 or len(clean_name) > 30:
            return False
        
        invalid_chars = ['|', '\t', '\n', '\r', '\x00']
        if any(char in clean_name for char in invalid_chars):
            return False
        
        return True

    def _find_similar_codes(self, target_code: str) -> List[str]:
        """å°‹æ‰¾ç›¸ä¼¼çš„å…¬å¸ä»£è™Ÿ"""
        if not self.watch_list_mapping:
            return []
        
        similar_codes = []
        target_num = None
        
        try:
            target_num = int(target_code)
        except ValueError:
            return similar_codes
        
        for code in self.watch_list_mapping.keys():
            try:
                code_num = int(code)
                if abs(code_num - target_num) <= 10:
                    similar_codes.append(code)
            except ValueError:
                continue
        
        return sorted(similar_codes)

    def _validate_watch_list_content_enhanced(self, mapping: Dict[str, str]):
        """v3.6.1 å¢å¼·çš„è§€å¯Ÿåå–®å…§å®¹é©—è­‰"""
        if not mapping:
            return
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¸¸è¦‹çš„æ¸¬è©¦å…¬å¸
        test_companies = {
            '2330': 'å°ç©é›»',
            '2317': 'é´»æµ·', 
            '2454': 'è¯ç™¼ç§‘',
            '2882': 'åœ‹æ³°é‡‘',
            '2412': 'ä¸­è¯é›»'
        }
        
        found_test_companies = 0
        for code, expected_name in test_companies.items():
            if code in mapping:
                actual_name = mapping[code]
                name_match = self._compare_company_names_enhanced(actual_name, expected_name)
                if name_match['is_match']:
                    found_test_companies += 1
                    print(f"æ‰¾åˆ°æ¸¬è©¦å…¬å¸: {code} - {actual_name} (åŒ¹é…é¡å‹: {name_match['match_type']})")
                else:
                    print(f"æ¸¬è©¦å…¬å¸åç¨±ä¸ç¬¦: {code} - æœŸæœ›:{expected_name}, å¯¦éš›:{actual_name}")
        
        # çµ±è¨ˆåˆ†æ
        code_ranges = self._analyze_code_ranges(mapping)
        print(f"è§€å¯Ÿåå–®ä»£è™Ÿåˆ†å¸ƒ: {code_ranges}")
        
        if found_test_companies == 0:
            print("æœªæ‰¾åˆ°ä»»ä½•å·²çŸ¥æ¸¬è©¦å…¬å¸ï¼Œè«‹æª¢æŸ¥è§€å¯Ÿåå–®å…§å®¹")
        else:
            print(f"æ‰¾åˆ° {found_test_companies}/{len(test_companies)} å€‹æ¸¬è©¦å…¬å¸")

    def _analyze_code_ranges(self, mapping: Dict[str, str]) -> Dict[str, int]:
        """åˆ†æå…¬å¸ä»£è™Ÿç¯„åœåˆ†å¸ƒ"""
        ranges = {
            '1000-1999': 0,
            '2000-2999': 0, 
            '3000-3999': 0,
            '4000-4999': 0,
            '5000-5999': 0,
            '6000-6999': 0,
            '7000-7999': 0,
            '8000-8999': 0,
            '9000-9999': 0
        }
        
        for code in mapping.keys():
            try:
                code_num = int(code)
                range_key = f"{(code_num // 1000) * 1000}-{(code_num // 1000) * 1000 + 999}"
                if range_key in ranges:
                    ranges[range_key] += 1
            except ValueError:
                continue
        
        return ranges

    # Keep all other existing methods unchanged
    def _extract_content_date_bulletproof(self, content: str) -> Optional[str]:
        """çµ•å°é˜²å½ˆçš„æ—¥æœŸæå– - æ’é™¤ YAML frontmatter"""
        actual_content = self._get_content_without_yaml(content)
        found_dates = []
        
        for i, pattern in enumerate(self.date_patterns):
            matches = re.findall(pattern, actual_content, re.MULTILINE | re.DOTALL)
            
            if matches:
                for match in matches:
                    try:
                        if len(match) >= 3:
                            year, month, day = match[0], match[1], match[2]
                            
                            if self._validate_date(year, month, day):
                                date_str = f"{year}/{int(month)}/{int(day)}"
                                confidence = self._calculate_date_confidence(pattern, match, actual_content, i)
                                
                                found_dates.append({
                                    'date': date_str,
                                    'pattern_index': i,
                                    'pattern': pattern,
                                    'confidence': confidence,
                                    'match': match
                                })
                                
                    except (ValueError, IndexError) as e:
                        continue
        
        if found_dates:
            found_dates.sort(key=lambda x: x['confidence'], reverse=True)
            best_date = found_dates[0]
            return best_date['date']
        
        return None

    def _extract_md_date_from_meta(self, content: str) -> Optional[str]:
        """Extract md_date from structured meta/JSON-LD/time tags (preferred)"""
        actual_content = self._get_content_without_yaml(content)

        meta_patterns = [
            r'property=["\']article:published_time["\']\s+content=["\'](\d{4})/(\d{1,2})/(\d{1,2})',
            r'property=["\']article:published_time["\']\s+content=["\'](\d{4})-(\d{1,2})-(\d{1,2})',
            r'"datePublished"\s*:\s*["\'](\d{4})-(\d{1,2})-(\d{1,2})',
            r'"datePublished"\s*:\s*["\'](\d{4})/(\d{1,2})/(\d{1,2})',
            r'<time[^>]*dateTime=["\'](\d{4})-(\d{1,2})-(\d{1,2})',
        ]

        for pattern in meta_patterns:
            match = re.search(pattern, actual_content, re.IGNORECASE)
            if match and len(match.groups()) >= 3:
                year, month, day = match.group(1), match.group(2), match.group(3)
                if self._validate_date(year, month, day):
                    return f"{year}/{int(month):02d}/{int(day):02d}"

        return None

    def _get_content_without_yaml(self, content: str) -> str:
        """ç§»é™¤ YAML frontmatterï¼Œåªè¿”å›å¯¦éš›å…§å®¹"""
        try:
            if content.startswith('---'):
                end_pos = content.find('---', 3)
                if end_pos != -1:
                    actual_content = content[end_pos + 3:].strip()
                    return actual_content
        except Exception as e:
            pass
        return content

    def _validate_date(self, year: str, month: str, day: str) -> bool:
        """é©—è­‰æ—¥æœŸçš„åˆç†æ€§"""
        try:
            y, m, d = int(year), int(month), int(day)
            if not (2020 <= y <= 2030):
                return False
            if not (1 <= m <= 12):
                return False
            if not (1 <= d <= 31):
                return False
            datetime(y, m, d)
            return True
        except (ValueError, TypeError):
            return False

    def _calculate_date_confidence(self, pattern: str, match: tuple, content: str, pattern_index: int) -> float:
        """è¨ˆç®—æ—¥æœŸåŒ¹é…çš„å¯ä¿¡åº¦"""
        confidence = 5.0
        
        if pattern_index == 0:
            confidence += 6.0
        elif pattern_index == 1:
            confidence += 5.5
        elif pattern_index == 2:
            confidence += 5.0
        elif pattern_index <= 6:
            confidence += 4.0
        elif 'cmoney' in content.lower() or 'CMoney' in content:
            confidence += 2.5
        elif 'é‰…äº¨ç¶²' in pattern:
            confidence += 2.0
        elif 'å¹´' in pattern and 'æœˆ' in pattern:
            confidence += 1.5
        elif '-' in pattern:
            confidence += 1.0
        
        match_text = ''.join(match)
        position = content.find(match_text)
        if position != -1:
            if position < len(content) * 0.1:
                confidence += 2.0
            elif position < len(content) * 0.3:
                confidence += 1.0
        
        try:
            year = int(match[0])
            current_year = datetime.now().year
            if year == current_year:
                confidence += 1.5
            elif year == current_year - 1:
                confidence += 1.0
        except:
            pass
        
        return confidence

    def _extract_file_info(self, file_path: str) -> Dict[str, Any]:
        """å¾æª”æ¡ˆè·¯å¾‘æå–åŸºæœ¬è³‡è¨Š"""
        filename = os.path.basename(file_path)
        name_without_ext = filename.replace('.md', '')
        parts = name_without_ext.split('_')
        
        result = {
            'company_code': None,
            'company_name': None,
            'data_source': None,
            'timestamp': None,
            'parsed_timestamp': None
        }
        
        if len(parts) >= 2:
            if parts[0].isdigit() and len(parts[0]) == 4:
                result['company_code'] = parts[0]
            
            if len(parts) > 1:
                result['company_name'] = parts[1]
            
            if len(parts) > 2:
                result['data_source'] = parts[2]
        
        return result

    def _extract_table_years(self, table_html: str) -> List[str]:
        """å¾è¡¨æ ¼æ¨™é¡Œåˆ—æå–å¹´ä»½åˆ—è¡¨"""
        try:
            # å°‹æ‰¾ç¬¬ä¸€å€‹ tr (æ¨™é¡Œåˆ—)
            first_row_match = re.search(r'<tr>(.*?)</tr>', table_html, re.DOTALL | re.IGNORECASE)
            if not first_row_match:
                return []
            
            row_content = first_row_match.group(1)
            # ç§»é™¤æ‰€æœ‰ HTML æ¨™ç±¤
            clean_row = re.sub(r'<[^>]+>', '', row_content)
            # å°‹æ‰¾æ‰€æœ‰ 4 ä½æ•¸å­— (å¹´ä»½)
            years = re.findall(r'(\d{4})', clean_row)
            
            # éæ¿¾åˆç†çš„å¹´ä»½ç¯„åœ (2023-2030)
            valid_years = [y for y in years if 2023 <= int(y) <= 2030]
            
            # å»é‡ä¸¦ä¿æŒé †åº
            seen = set()
            ordered_years = []
            for y in valid_years:
                if y not in seen:
                    ordered_years.append(y)
                    seen.add(y)
            
            return ordered_years
        except Exception as e:
            print(f"âš ï¸ æå–è¡¨æ ¼å¹´ä»½å¤±æ•—: {e}")
            return []

    def _extract_eps_data(self, content: str) -> Dict[str, List[float]]:
        """æå– EPS è³‡æ–™"""
        eps_data = {'2025': [], '2026': [], '2027': [], '2028': []}

        table_stats = self._extract_eps_table_stats(content)
        if table_stats:
            eps_data['_table_stats'] = table_stats
        else:
            eps_data.update(self._extract_eps_from_table(content))

        for pattern in self.eps_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                try:
                    year = match[0]
                    value = float(match[1])

                    if year in eps_data and 0 < value < 1000:
                        eps_data[year].append(value)
                except (ValueError, IndexError):
                    continue

        for year in ['2025', '2026', '2027', '2028']:
            eps_data[year] = list(set(eps_data[year]))

        return eps_data

    def _extract_eps_from_table(self, content: str) -> Dict[str, List[float]]:
        """å¾è¡¨æ ¼ä¸­æå– EPS è³‡æ–™ (å¢å¼·ç‰ˆï¼šå‹•æ…‹å¹´ä»½)"""
        eps_data = {'2025': [], '2026': [], '2027': [], '2028': []}
        
        # 1. å®šä½è¡¨æ ¼ä¸¦æå–å¹´ä»½
        table_html = self._find_eps_table_html(content)
        if not table_html:
            return eps_data
            
        table_years = self._extract_table_years(table_html)
        if not table_years:
            return eps_data
            
        num_years = len(table_years)
        
        # 2. æ ¹æ“šå¹´ä»½æ•¸é‡æå–æ•¸æ“š
        # Markdown è¡¨æ ¼æ¨¡å¼
        if '|' in content:
            md_td_pattern = "".join([r'[^|]*\|\s*([0-9]+\.?[0-9]*)' for _ in range(num_years)])
            md_row_pattern = rf'\|\s*(æœ€é«˜å€¼|æœ€ä½å€¼|å¹³å‡å€¼|ä¸­ä½æ•¸){md_td_pattern}'
            
            for match in re.findall(md_row_pattern, content):
                label = match[0]
                values = match[1:]
                for year, val_str in zip(table_years, values):
                    if year in eps_data:
                        try:
                            value = float(val_str)
                            if 0 < value < 1000:
                                eps_data[year].append(value)
                        except: continue

        # HTML è¡¨æ ¼æ¨¡å¼
        td_patterns = "".join([r'<td[^>]*>([0-9,]+(?:\.[0-9]+)?)[^<]*</td>\s*' for _ in range(num_years)])
        html_row_pattern = rf'<tr>\s*<td[^>]*>(æœ€é«˜å€¼|æœ€ä½å€¼|å¹³å‡å€¼|ä¸­ä½æ•¸)</td>\s*{td_patterns}'
        
        for match in re.findall(html_row_pattern, table_html, re.IGNORECASE | re.DOTALL):
            label = match[0]
            values = match[1:]
            for year, val_str in zip(table_years, values):
                if year in eps_data:
                    try:
                        value = float(val_str.replace(',', ''))
                        if 0 < value < 1000:
                            eps_data[year].append(value)
                    except: continue
        
        return eps_data

    def _extract_eps_table_stats(self, content: str) -> Dict[str, Dict[str, float]]:
        """å¾ EPS è¡¨æ ¼æå–çµ±è¨ˆå€¼ (å‹•æ…‹å¹´ä»½)"""
        table_html = self._find_eps_table_html(content)
        if not table_html:
            return {}

        label_map = {'æœ€é«˜å€¼': 'high', 'æœ€ä½å€¼': 'low', 'å¹³å‡å€¼': 'avg', 'ä¸­ä½æ•¸': 'median'}
        stats: Dict[str, Dict[str, float]] = {}

        # å‹•æ…‹æå–å¹´ä»½
        table_years = self._extract_table_years(table_html)
        if not table_years:
            return {}

        num_years = len(table_years)
        # æ§‹å»ºå°æ‡‰å¹´ä»½æ•¸é‡çš„ td æ¨¡å¼
        td_patterns = "".join([r'\s*<td[^>]*>([^<]+)</td>' for _ in range(num_years)])
        row_pattern = rf'<tr>\s*<td[^>]*>(æœ€é«˜å€¼|æœ€ä½å€¼|å¹³å‡å€¼|ä¸­ä½æ•¸)</td>{td_patterns}'
        
        for match in re.findall(row_pattern, table_html, re.IGNORECASE | re.DOTALL):
            label = match[0]
            values = match[1:]
            for year, raw in zip(table_years, values):
                value = self._parse_numeric_value(raw)
                if value is not None:
                    stats.setdefault(year, {})[label_map[label]] = value

        return stats

    def _find_eps_table_html(self, content: str) -> Optional[str]:
        """å®šä½å¸‚å ´é ä¼° EPS è¡¨æ ¼"""
        eps_table_match = re.search(
            r'å¸‚å ´é ä¼°EPS.*?<table[^>]*>.*?</table>',
            content,
            re.DOTALL
        )
        if eps_table_match:
            return eps_table_match.group(0)
        return None

    def _find_revenue_table_html(self, content: str) -> Optional[str]:
        """å®šä½å¸‚å ´é ä¼°ç‡Ÿæ”¶è¡¨æ ¼"""
        revenue_table_match = re.search(
            r'å¸‚å ´é ä¼°ç‡Ÿæ”¶.*?<table[^>]*>.*?</table>',
            content,
            re.DOTALL
        )
        if revenue_table_match:
            return revenue_table_match.group(0)
        return None

    def _parse_numeric_value(self, value: str, min_val: float = 0) -> Optional[float]:
        """è§£æè¡¨æ ¼ä¸­çš„æ•¸å€¼"""
        value = re.sub(r'\([^)]*\)', '', value)
        value = value.replace(',', '').strip()
        try:
            number = float(value)
        except ValueError:
            return None
        
        if number <= min_val:
            return None
        return number

    def _extract_revenue_table_stats(self, content: str) -> Dict[str, Dict[str, float]]:
        """å¾ç‡Ÿæ”¶è¡¨æ ¼æå–çµ±è¨ˆå€¼ (å‹•æ…‹å¹´ä»½)"""
        table_html = self._find_revenue_table_html(content)
        if not table_html:
            return {}

        label_map = {'æœ€é«˜å€¼': 'high', 'æœ€ä½å€¼': 'low', 'å¹³å‡å€¼': 'avg', 'ä¸­ä½æ•¸': 'median'}
        stats: Dict[str, Dict[str, float]] = {}

        table_years = self._extract_table_years(table_html)
        if not table_years:
            return {}

        num_years = len(table_years)
        td_patterns = "".join([r'\s*<td[^>]*>([^<]+)</td>' for _ in range(num_years)])
        row_pattern = rf'<tr>\s*<td[^>]*>(æœ€é«˜å€¼|æœ€ä½å€¼|å¹³å‡å€¼|ä¸­ä½æ•¸)</td>{td_patterns}'
        
        for match in re.findall(row_pattern, table_html, re.IGNORECASE | re.DOTALL):
            label = match[0]
            values = match[1:]
            for year, raw in zip(table_years, values):
                value = self._parse_numeric_value(raw, min_val=1000)
                if value is not None:
                    stats.setdefault(year, {})[label_map[label]] = value

        return stats

    def _calculate_revenue_statistics(self, content: str) -> Dict[str, Any]:
        """è¨ˆç®—ç‡Ÿæ”¶çµ±è¨ˆè³‡æ–™"""
        result = {}
        table_stats = self._extract_revenue_table_stats(content)

        for year in ['2025', '2026', '2027', '2028']:
            year_stats = table_stats.get(year, {})
            result[f'revenue_{year}_high'] = year_stats.get('high')
            result[f'revenue_{year}_low'] = year_stats.get('low')
            result[f'revenue_{year}_avg'] = year_stats.get('avg')
            result[f'revenue_{year}_median'] = year_stats.get('median')

        return result

    def _extract_target_price(self, content: str) -> Optional[float]:
        """æå–ç›®æ¨™åƒ¹æ ¼"""
        for pattern in self.target_price_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                try:
                    price = float(matches[0])
                    if 0 < price < 10000:
                        return price
                except ValueError:
                    continue
        return None

    def _extract_analyst_count(self, content: str) -> int:
        """æå–åˆ†æå¸«æ•¸é‡"""
        for pattern in self.analyst_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                try:
                    count = int(matches[0])
                    if 0 < count < 1000:
                        return count
                except ValueError:
                    continue
        return 0

    def _calculate_eps_statistics(self, eps_data: Dict[str, List[float]]) -> Dict[str, Any]:
        """è¨ˆç®— EPS çµ±è¨ˆè³‡æ–™"""
        result = {}
        table_stats = eps_data.get('_table_stats', {})

        for year in ['2025', '2026', '2027', '2028']:
            values = eps_data.get(year, [])
            table_year_stats = table_stats.get(year, {})

            high = table_year_stats.get('high')
            low = table_year_stats.get('low')
            avg = table_year_stats.get('avg')
            median = table_year_stats.get('median')

            if high is None and values:
                high = max(values)
            if low is None and values:
                low = min(values)
            if avg is None and values:
                avg = round(statistics.mean(values), 2)
            if median is None and values:
                median = round(statistics.median(values), 2)

            result[f'eps_{year}_high'] = high
            result[f'eps_{year}_low'] = low
            result[f'eps_{year}_avg'] = avg
            result[f'eps_{year}_median'] = median

        return result

    def _get_debug_info_enhanced(self, content: str, extracted_date: Optional[str], 
                                search_keywords: List[str]) -> Dict[str, Any]:
        """v3.6.1 å¢å¼·çš„èª¿è©¦è³‡è¨Š"""
        return {
            'content_preview': content[:200] + "..." if len(content) > 200 else content,
            'extracted_date': extracted_date,
            'yaml_detected': content.startswith('---'),
            'content_length': len(content),
            'search_keywords_count': len(search_keywords),
            'search_keywords_preview': search_keywords[:5],
            'watch_list_loaded': self.validation_enabled,
            'watch_list_size': len(self.watch_list_mapping),
            'parser_version': self.version,
            'content_structure': {
                'has_metadata': content.startswith('---'),
                'paragraph_count': len(content.split('\n\n')),
                'line_count': len(content.split('\n')),
                'chinese_detected': bool(re.search(r'[\u4e00-\u9fff]', content))
            }
        }

    def _create_empty_result_enhanced(self, file_path: str, error_msg: str) -> Dict[str, Any]:
        """v3.6.1 å¢å¼·çš„ç©ºçµæœå»ºç«‹"""
        file_info = self._extract_file_info(file_path)
        
        return {
            'filename': os.path.basename(file_path),
            'company_code': file_info.get('company_code'),
            'company_name': file_info.get('company_name'),
            'data_source': file_info.get('data_source'),
            'file_mtime': datetime.fromtimestamp(os.path.getmtime(file_path)) if os.path.exists(file_path) else None,
            'content_date': None,
            'eps_2025_high': None, 'eps_2025_low': None, 'eps_2025_avg': None,
            'eps_2026_high': None, 'eps_2026_low': None, 'eps_2026_avg': None,
            'eps_2027_high': None, 'eps_2027_low': None, 'eps_2027_avg': None,
            'eps_2028_high': None, 'eps_2028_low': None, 'eps_2028_avg': None,
            'target_price': None,
            'analyst_count': 0,
            'has_eps_data': False,
            'has_target_price': False,
            'has_analyst_info': False,
            'data_richness_score': 1.0,  # MODIFIED: Low score for error cases
            'quality_score': 1.0,        # MODIFIED: Low score for error cases
            'search_keywords': [],  # é‡è¦ï¼
            'content_quality_metrics': {},
            'yaml_data': {},
            'content': '',
            'content_length': 0,
            'parsed_at': datetime.now(),
            'parser_version': self.version,
            'error': error_msg,
            'date_extraction_method': 'error',
            'validation_result': {
                'overall_status': 'error', 
                'errors': [error_msg],
                'validation_method': 'error_state'
            },
            'content_validation_passed': False,
            'validation_warnings': [],
            'validation_errors': [error_msg],
            'validation_enabled': self.validation_enabled
        }


# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    parser = MDParser()
    
    print(f"=== MD Parser v{parser.version} æ¸¬è©¦ (å¢å¼·ç‰ˆå“è³ªè©•åˆ†) ===")
    print(f"è§€å¯Ÿåå–®è¼‰å…¥: {len(parser.watch_list_mapping)} å®¶å…¬å¸")
    print(f"é©—è­‰åŠŸèƒ½: {'å•Ÿç”¨' if parser.validation_enabled else 'åœç”¨'}")
    
    if parser.validation_enabled:
        # æ¸¬è©¦å¢å¼·ç‰ˆé©—è­‰é‚è¼¯
        test_cases = [
            ('6462', 'ase'),      # éŒ¯èª¤åç¨±
            ('6811', 'fubon'),    # éŒ¯èª¤åç¨±  
            ('9999', 'ä¸å­˜åœ¨'),    # ä¸å­˜åœ¨çš„ä»£è™Ÿ
            ('abc', 'test'),      # ç„¡æ•ˆæ ¼å¼
            ('2330', 'å°ç©é›»')     # æ­£å¸¸å…¬å¸ (å¦‚æœåœ¨è§€å¯Ÿåå–®ä¸­)
        ]
        
        print(f"\næ¸¬è©¦å¢å¼·ç‰ˆé©—è­‰:")
        for code, name in test_cases:
            result = parser._validate_against_watch_list_enhanced(code, name)
            status = result['overall_status']
            errors = len(result.get('errors', []))
            confidence = result.get('confidence_score', 0)
            method = result.get('validation_method', 'unknown')
            checks = len(result.get('detailed_checks', []))
            
            print(f"  {code} ({name}): {status} - ä¿¡å¿ƒåº¦:{confidence} - æ–¹æ³•:{method} - æª¢æŸ¥:{checks} - éŒ¯èª¤:{errors}")
            
            if errors > 0:
                for error in result.get('errors', [])[:1]:  # åªé¡¯ç¤ºç¬¬ä¸€å€‹éŒ¯èª¤
                    print(f"    {error}")
    else:
        print("è§€å¯Ÿåå–®é©—è­‰å·²åœç”¨")
    
    # æ¸¬è©¦æŸ¥è©¢æ¨¡å¼æå–
    print(f"\næ¸¬è©¦æŸ¥è©¢æ¨¡å¼æå–:")
    test_content = '''---
search_query: å°ç©é›» 2330 factset eps é ä¼°
keywords: åŠå°é«”, æ™¶åœ“ä»£å·¥, å°ç©é›», factset
original_query: "å°ç©é›»" factset åˆ†æå¸« ç›®æ¨™åƒ¹
---

å°ç©é›»ç¬¬ä¸‰å­£è²¡å ±åˆ†æ...
'''
    
    test_yaml = {
        'search_query': 'ç¥ç›¾ 6462 factset ç”Ÿç‰©è¾¨è­˜',
        'keywords': 'ç¥ç›¾, factset, æŒ‡ç´‹è¾¨è­˜'
    }
    
    keywords = parser._extract_search_keywords_enhanced(test_content, test_yaml)
    print(f"   æå–çš„é—œéµå­—: {keywords}")
    print(f"   é—œéµå­—æ•¸é‡: {len(keywords)}")
    
    # æ¸¬è©¦å“è³ªè©•åˆ† (ç¼ºå°‘å…§å®¹æ—¥æœŸ)
    print(f"\næ¸¬è©¦å“è³ªè©•åˆ† - ç¼ºå°‘å…§å®¹æ—¥æœŸ:")
    eps_stats = {'eps_2025_avg': 50.0, 'eps_2026_avg': 55.0, 'eps_2027_avg': None}
    target_price = 600.0
    analyst_count = 25
    
    # æœ‰å…§å®¹æ—¥æœŸçš„æƒ…æ³
    quality_with_date = parser._calculate_data_richness_enhanced(eps_stats, target_price, analyst_count, "2025/06/24")
    print(f"   æœ‰å…§å®¹æ—¥æœŸ: {quality_with_date}")
    
    # ç¼ºå°‘å…§å®¹æ—¥æœŸçš„æƒ…æ³
    quality_without_date = parser._calculate_data_richness_enhanced(eps_stats, target_price, analyst_count, "")
    print(f"   ç¼ºå°‘å…§å®¹æ—¥æœŸ: {quality_without_date}")
    
    print(f"\nv{parser.version} å¢å¼·ç‰ˆ MD Parser å·²å•Ÿå‹•ï¼")
    print(f"æ–°åŠŸèƒ½: å°ç¼ºå°‘å…§å®¹æ—¥æœŸçš„æª”æ¡ˆçµ¦äºˆä½å“è³ªè©•åˆ† (â‰¤1åˆ†)")
    print(f"ä¸»è¦ä¿®æ­£: å¢å¼·å“è³ªè©•åˆ†é‚è¼¯ï¼Œç¢ºä¿è²¡å‹™è³‡è¨Šæœ‰æ•ˆæ€§")
